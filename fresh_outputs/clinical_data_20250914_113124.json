{
  "title": "Investigating the Linguistic Performance of Large Language Models",
  "participants": "1,200 participants",
  "study_type": "Unable to determine study type",
  "endpoints": "Grammar, agreement phenomena, negation, syntactic states, hierarchical organization of words\n\n{'primary': 'Accuracy and response stability of LLMs in grammaticality judgment tasks', 'secondary': 'Comparison of human and LLM performance on grammaticality judgment tasks'}\n\n{'primary': 'Accuracy and stability of LLMs on a grammaticality judgment task', 'secondary': 'Comparison of LLMs to human subjects'}\n\n{'primary': 'Evaluating the performance of LLMs in manipulating language and interacting with humans', 'secondary': 'Comparing the similarities and differences between LLMs and humans in processing language'}\n\n{'primary': 'Accuracy of language models in grammatical and ungrammatical sentences', 'secondary': 'Stability of language models in grammatical and ungrammatical sentences'}\n\n{'primary': 'Accuracy and stability of large language models', 'secondary': 'Effects of model size and repetitions on accuracy and stability'}\n\n{'primary': 'Accuracy of grammatical and ungrammatical sentences', 'secondary': 'Response stability, oscillations, and deviations'}\n\n{'primary': 'Stability and accuracy of language models in identifying grammatical errors', 'secondary': 'Comparison of language models to human language abilities'}\n\n{'primary': 'To determine whether LLMs possess human-like language capacities', 'secondary': 'To assess the capacity of LLMs to encode grammaticality'}\n\n{'primary': 'Grammaticality judgment task', 'secondary': 'Comparison of LLM and human performance'}\n\nNo information available\n\nNo information available\n\nNo information available\n\nNot specified",
  "results_summary": "of the best performing LLM, ChatGPT-4, are compared to results of n = 80 humans on the same stimuli\n\nThe study found mixed evidence on the ability of LLMs to handle agreement phenomena, with some models struggling to succeed at subject-verb number agreement prediction. However, a model trained on language modeling was able to formulate largely accurate number agreement predictions. The study also found that LLMs can handle hierarchical language phenomena, including the ability to track syntactic states and develop neural units sensitive to innovation.\n\nThe study found that LLMs struggled in providing consistent, accurate judgments, especially for ungrammatical sentences, marking a stark difference from human performance. The present work investigates whether model scaling mitigates such differences.\n\nThe study found that larger LLMs (ChatGPT-4) performed better than smaller LLMs (Bard and ChatGPT-3.5) on the grammaticality judgment task, but the results were affected by the exposure of the models to the testing materials.\n\nOverall, the study found that LLMs can be evaluated on their default capacity to manipulate language and interact with humans, and that prompting LLMs with grammaticality judgment tasks can inform the field of linguistics.\n\n{'key_findings': 'ChatGPT-4 outperforms other language models in accuracy and stability, especially in ungrammatical sentences.', 'results': 'ChatGPT-4 reaches a high level of accuracy (93.5%) for grammatical sentences and performs above chance level for ungrammatical sentences.'}\n\n{'key_findings': 'The study found that model size does not necessarily correlate with performance improvement over repetitions. Some models tend to improve for some conditions over repetitions, while others decline in accuracy.', 'results': 'ChatGPT-4 showed improvement in a condition where ChatGPT-3.5 worsened. The study also found no evidence that response stability changes over repeated presentations of the same sentence.'}\n\nChatGPT-4 achieves higher accuracy than humans, but performs poorly on ungrammatical sentences and has less stable responses.\n\nshow that even the best-performing model in our analyses, ChatGPT-4, does not behave comparably to humans: ChatGPT-4’s accuracy rate in grammatical sentences surpasses that of human subjects, but it is lower than that of humans on ungram- matical stimuli, where it further decreases upon repeated exposure\n\nThe study found that while language models (LLMs) surpass humans in accuracy for the grammatical condition, they underperform in the ungrammatical condition and fluctuate in their responses. These findings are difficult to reconcile with the proposed idea that LLMs run counter to claims of language innateness in humans.\n\nThe study found that LLMs fail to fully reproduce human linguistic behavior, but this is not evidence for innateness in humans or support for any specific theory of innateness. The results suggest that LLMs do not possess the mastery of language at their current stage of development.\n\n```json\n{\n  \"title\": \"Comparability of Human and LLM Evaluation Methods\",\n  \"participants\": \"No specific information provided\",\n  \"study_type\": \"Observational study\",\n  \"endpoints\": {\n    \"primary\": \"Comparability of human and LLM evaluation methods\",\n    \"secondary\": \"Language abilities of humans vs. models\"\n  },\n  \"results_summary\": \"LLMs have access to negative evidence, but still fail to identify ungrammatical sentences in some cases.\",\n  \"methodology\": \"Comparative study of human and LLM evaluation methods\",\n  \"adverse_events\": \"No adverse events reported\",\n  \"statistical_analysis\": \"No statistical analysis provided\"\n}\n```\n\nhereby presented challenge this view: the inability of the tested LLMs to adhere to a grammaticality judgment task’s demands suggests that LLMs lack the internal mechanisms that allow humans to naturally tell grammatical and ungrammatical stimuli apart [79]\n\nThe results showed that ChatGPT-4 outperformed humans in one experimental condition, but did not perform better in the ungrammatical condition. The study found that LLMs' linguistic behavior is not comparable to that of humans, despite the vast amount of data they are trained on.\n\nThe study found that large language models can dissociate language and thought, but the results are not conclusive.\n\nNo information available\n\nThe study explores the ability of language models to follow instructions with human feedback.\n\nThe study discusses the limitations of large language models in understanding human language and the world behind words.",
  "methodology": "The study used a combination of language modeling and agreement prediction tasks to investigate the linguistic performance of LLMs. The authors trained and tested several models on different datasets, including those with illicit structural relations.",
  "adverse_events": "No information available\n\nNot applicable\n\nNone reported\n\nNone reported\n\nNot applicable\n\n{'safety_data': 'Not specified', 'adverse_events': 'Not specified'}\n\nNot applicable\n\nNot applicable\n\nNo adverse events reported\n\nNo information about adverse events is provided\n\nNo information available\n\nNo information available\n\nNo information available\n\nNot applicable",
  "statistical_analysis": "The study used statistical methods to analyze the performance of the LLMs on the agreement prediction tasks. The authors found that the performance of the models was significantly affected by the properties of the training corpora."
}